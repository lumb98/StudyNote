# 神经网络

全连接神经网络

归一化：目的是将上一步计算的结果根据一个激活函数，映射到一定范围内，以此来避免一些超大或者超小的数值导致后续计算的有效性造成影响。

反向传播：反向传播传播的是什么呢，因为神经网络的训练是有监督学习，所以我们在训练时，输入的一个值X，是有一个明确且正确的输出值Y的，而未完成训练的神经网络所输出的Y就会喝真实值Y有一定差别，它们之间的损失loss就是反向传播的东西，而整个网络训练的过程也就是不断调整权重，从而缩小loss的过程。

MSE(Mean Squared Error)均方误差：





#  pytorch

GPU和CPU拥有不同tensor

`a=torch.randn(2,3)`随机生成一个两行三列的tensor赋给a

可以用`type`方法或函数来确认某数据的类型，如上述随机生成的tensor，我们就可以用`a.type()`和`type(a)`，调用这两个函数会输出这a的类型。

在代码运行时可以使用`isinstance(a,torch.FloatTensor)`来确认变量a是否是FloatTensor类型，

Tensor类型在pytorch中是有普通的tensor类型和`cuda.tensor`类型的，我们可以利用，cuda方法，将普通的tensor类型转换成`cuda.tensor`,实际上它也不是发生了转化，只是返回了一个GPU上的引用。

```python
data=torch.randn(10,10)
isinstance(data,torch.cuda.FloatTensor)
#output:False
data=data.cuda()
isinstance(data,torch.cuda.FloatTensor)
#output:True
```

可以利用`torch.tensor()`方法将数据转换为tensor类型，

```python
torch.tensor(1.)
#out:tensor(1.) ##类型为float
torch.tensor(1.3)
#out:tensor(1.300) ##类型也float
torch.tensor(1) ##类型为long
```

上述的输入中tensor的维度都是0维(dimensionality)，若在tensor的括号中加入中括号，`torch.tensor([1])`那么tensor就会变成1维，我们可以查看成员`shape`，来查看该tensor的维度，我们可以`len(a.shapr)`来查看其返回值，该值就是变量`a`的维度。同样地，`a.size()`方法也会返回维度。

 我们可以用`torch.FloatTensor(m,n)`来随机创建一个float类型的长度为m*n的张量（Tensor）

我们也可以用numpy来创建一个数组然后再将其转换为张量。

```python
data=numpy.ones(2)#随机生成一个长度为2的向量并赋给data
data=torch.from_numpy(data)#将data转换为张量类型，并赋给data。
isinstance(data,torch.DoubleTensor)#查看data是否为torch.DoubleTensor类型
#out：True
```

张量的度量方式

如何区分dimensionality、size和shape：

1. dim为tensor的维度，`data.dim()`会返回数据的维度
2. size()和shape是tensor的形状，也就是它各个维度的大小。

```python
a=torch.rand(2,3,4,5)
a.dim()#tensor的维度
#out:4
a.numel()#tensor中元素数量
#out:120
a.shape()#tensor的形状
#out:[2,3,4,5]
```



张量的创建

首先可以将numpy的数据类型转换为tensor

```python
a=numpy.array([2,3.3])#创建一个数组[2,3.3]
torch.from_numpy(a)#将数组转换为tensor

a=numpy.ones([2,3])#生成一个2*3的矩阵
torch.from_numpy(a)#将矩阵转换为tensor
```

利用tensor创建

```python
torch.tensor([2.,3.2])#创建一个一维的tensor，内含数组[2,3.2]
torch.FloatTensor([2.,3.2])#创建一个一维的tensor，内含数组[2,3.2]
torch.tensor([2.,3.2],[1.,4.5])#创建一个二维的tensor，内含数组矩阵[2.,3.2],[1.,4.5]

torch.Tensor(2,3)#生成2*3的tensor
torch.Tensor([2,3])#生成一个tensor 包含向量[2,3]
```

需要注意的是`torch.tensor()和torch.Tensor()`是不同的他们之间有区别：

* 小写的tensor是接受现成的数据，
* 大写的Tensor是接收shape，但Tensor实际上也可以接受数据但是这个数据必须用list表示，FloatTensor也是同理的

tensor的初始化

我们使用shape对tensor进行创建的时候，tensor一般是未初始化状态，可能是正负无穷大也可能是无限趋近于零，不初始化很容易造成程序的隐患，我们一般会使用`rand/rand_like`和`randint`对tensor进行初始化:

```python
a=torch.rand(3,3)#随机初始化一个shape为（3，3）的tensor,并赋值给a
b=torch.rand_like(a)#随机初始化一个shape和a一样的tensor，实际上是在内部调用了a.shape
torch.randint(1,10,[3,3])#随机初始化一个int类型的tensor，shape为3*3，每个元素为1~10

torch.randn(3,3)#创建一个shape为3*3的正态分布的tensor，默认的是N(0,1)均值为0，方差为1
torch.normal(mean=torch.full([10],0),std=torch.arange(1,0,-0.1))
#在上述代码中torch.full([10],0)是创建一个长度为10的向量里面的每个元素都是0
#torch.arange(1,0,-0.1)是创建一个tensor向量，从1开始一直到0（不包含0），每次递减0.1
#将上述生成的分别赋值给均值mean和方差std，作为正太分布的参数，生成
torch.full([2,3],7)# 生成两行三列的7
torch.full([],7)#生成标量7
torch.full([1],7)#生成一个一维tensor有一个元素7

torch.arange(0,10)#生成一个tensor 为0~9

torch.arange(0,10,2)#生成一个tensor 为0,2,4,6,8

torch.linspace(1,10,step=4)#生成0~10之内均分的tensor，共有四个元素[0,3.3,6.6,10]

torch.logspace(0,10,steps=11,base=10)#生成10的0次方到10次方之间的数，steps为均分的次数。
#out：tensor([1.0000e+00, 1.0000e+01, 1.0000e+02, 1.0000e+03, 1.0000e+04, 1.0000e+05,
       # 1.0000e+06, 1.0000e+07, 1.0000e+08, 1.0000e+09, 1.0000e+10])
torch.ones(3,3)#生成一个3*3的矩阵，内容全部是1
torch.zeros(3,3)#生成一个3*3的矩阵，内容全部是0
torch.eye(3,4)#生成一个3*4的对角矩阵。
'''	tensor([[1., 0., 0., 0.],
        	[0., 1., 0., 0.],
        	[0., 0., 1., 0.]])'''
torch.ones_like(a)#生成一个全是1的和a的shape相同的矩阵。
#其他的也能用like，同理.
idx=torch.randperm(100)#随机生成一个0~99的向量。共包含0~99一百个元素。
a=torch.rand(100,4)
b=torch.rand(100,4)
a[idx]
b[idx]
#利用上述做法，我们就可以对a和b中的元素进行随机访问了。
```

torch的索引与切片



```python
a=torch.rand(4,3,28,28)#假设这是四张RGB的28*28的方形图片
#对第一维索引,即选择的是第一张图片
a[0].shape
#对第二维度索引，即选择第一张图片的第一个通道红色通道，
a[0,0].shape
#对后续维度进行索引，即打印一个像素点的值,回返回一个标量
a[0,0,2,3]

a[:2]#表示从第0张图片到第1张图片
a[:2,:1,:,:]#表示从第0~1张图片的第0~0个通道取所有像素。
a[1:]#表示从第1张图片到最后一张图片，注意不是第0张开始
a[-1:]#表示从最后一个开始取，取到第0个
#start:end:steps
a[:,:,0:28:2,0:28:2]#对所有图片的所以通道取0~27的数据，0，2，4...26
a[:,:,::2,::2]#对所有图片的所以通道取的所有像素，间隔取值，0，2，4...26

a.index_select(0,[0,2])#指第0个维度，第0张图片和第2张图片
a.index_select(1,[1,2])#指第1个维度，的第1个通道和第2个通道的数据，即所有图片的GB通道数据
#需要注意的是第二个参数必须是tensor类型。
a.index_select(2,torch.arange(8))#取每个图片的8*28

a[0,...]#表示取第0张图片的数据
a[:,1,...]#表示取所有图片的第1个通道
a[...,:2]#表示所有图片的三通道28*2
#实际上就是代替了很多个：：：

mask=a.ge(0.5)#所有大于0.5的元素变为1，小于等于的变为0
torch.masked_select(a,mask)#会将x对于掩码中1的元素平铺

torch.take(a,torch.tensor([0,2,5]))#take会将多有元素打回一维然后按照0,2,5的索引将元素取出。

```



torch的维度变换

view

```python
a=torch.rand(4,3,2)
b=a.view(4,3*2)#此时b会变成shape为4*6的tensor，也就是将tensor展开了
c=b.view(4,3,2)# 与a相等
```

需要注意的是view方法需要tensor的内存是连续的，才能将tensor展开，我们可以用`is_contiguous`方法来判断该tensor的内存是不是连续的，如果是连续的则会返回true，否则回返回false，我们可以用`contiguous`方法将tensor的内存变得连续。`contiguos`内存连续是行优先的。

squeze expand

```python
a=torch.rand(4,3,28,28)
a.unsqueeze(0).shape#在当前0的位置前插入一个维度
a.unsqueeze(-1).shape#在当前倒数第一的位置后插入一个维度

b=torch.rand(32)
f=torch.rand(4,32,14,14)
b=b.unsqueeze(1).unsqueeze(2).unsqueeze(0)
b.shape
# out:torch.Size([1, 32, 1, 1])

b.squeeze()#会将所有shape为1的维度压缩。
b.squeeze(0)#若第0个维度的shape为1，则将第0个维度压缩
b.squeeze(-1)#若倒数第1个维度的shape为1，则将倒数第1个维度压缩

# out:torch.Size([1, 32, 1, 1])
b.expand(4,32,14,14).shape#将对应维度扩展到对应的shape,注意，不能缩小
# out:torch.Size([4, 32, 14, 14])
b.expand(-1,32,-1,-1).shape#目标shape为-1的意味不对该维度进行操作

# out: torch.Size([1, 32, 1, 1])
b.repeat(4,32,1,1).shape # 表示对四个维度分别拷贝4、32、1、1次
# out: torch.Size([4, 1024, 1, 1])
a.torch.rand(2,3,4,5)
a.repeatt(2,2,2,2).shape
# out: torch.Size([4, 6, 8, 10])

a.torch.rand(2,3,4,5)
a.repeatt(2,1,1,2).shape
# out: torch.Size([4, 3, 4, 10])

#转置操作
b.t()#可以对b矩阵进行转置，但是需要注意的是该方法只能用于2D矩阵。

# out: torch.Size([1, 32, 1, 1])
b.transpose(0,1).shape #交换第0和第1维度
#out: torch.Size([32, 1, 1, 1])

# out: torch.Size([1, 2, 3, 4])
b.permute(0,2,3,1).shape #根据当前的index，来指定目标顺序。会打乱内存顺序，需要调用contiguos
# out: torch.Size([1, 3, 4, 2])
```

Broadcasting 自动扩展机制

```python
a=torch.tensor([[0],[10],[20],[30]])
b=torch.tensor([0,1,2])
c=a+b
print(c)
#out:
'''
tensor([[ 0,  1,  2],
        [10, 11, 12],
        [20, 21, 22],
        [30, 31, 32]])
'''
```





Tensor的拼接和拆分

```python
#cat 剪切 拼接
a1=torch.rand(4,3,32,32)
a2=torch.rand(5,3,32,32)
torch.cat([a1,a2],dim=0).shape#从第0个维度将a1和a2两个tensor进行拼接
#out:torch.Size([9, 3, 32, 32])

a1=torch.rand(4,3,16,32)
a2=torch.rand(4,3,16,32)
torch.cat([a1,a2],dim=2).shape#从第2个维度将a1和a2两个tensor进行拼接
#out:torch.Size([4, 3, 32, 32])

#stack 堆叠
a1=torch.ones(4,3,32,32)
a2=torch.zeros(4,3,32,32)
a3=torch.stack([a1,a2],dim=2)#在第2维度处创建一个新的维度，将a1和a2在的第2维度进行堆叠，即[:,:,0,...]就是a1全为1，[:,:,1,...]则为a2全为0
#out:torch.Size([4, 3, 2, 32, 32])

a1=torch.ones(4,3)
a2=torch.zeros(4,3)
torch.stack([a1,a2],dim=0).shape
#out: torch.Size([2, 4, 3])

#split 分割
a3=torch.eye(4,3)
a=torch.stack([a1,a2,a3],dim=0)

b1,b2,b3=a.split(1,dim=0)#将a在第0维，每1个拆分成一个维度，即拆分成三个[1,4,3]
print(b1)
print(b2)
print(b3)
'''out
tensor([[[1., 1., 1.],
         [1., 1., 1.],
         [1., 1., 1.],
         [1., 1., 1.]]])
tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]])
tensor([[[1., 0., 0.],
         [0., 1., 0.],
         [0., 0., 1.],
         [0., 0., 0.]]])
'''
b1,b2=a.split([2,1],dim=0)#将a，在第0维，按照[2,1]拆分成两个tensor，即前两个是一个维度，最后是一个维度
print(b1)
print(b2)

'''out
tensor([[[1., 1., 1.],
         [1., 1., 1.],
         [1., 1., 1.],
         [1., 1., 1.]],

        [[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]])
tensor([[[1., 0., 0.],
         [0., 1., 0.],
         [0., 0., 1.],
         [0., 0., 0.]]])
'''

#chunk 
b1=a.chunk(1,dim=0)#将a，在第0维，将tensor分成一份
print(b1)

b1,b2,b3=a.chunk(3,dim=0)#将a，在第0维，将tensor分成三份
print(b1),print(b2),print(b3)
```



Tensor的基本运算

加减乘除

```python
a=torch.rand(3,4)
b=torch.rand(4)
c=a+b
c.shape
#out: [3,4]
torch.all(torch.eq(a+b,torch.add(a,b)))
torch.all(torch.eq(a-b,torch.sub(a,b)))
torch.all(torch.eq(a*b,torch.mul(a,b)))
torch.all(torch.eq(a/b,torch.div(a,b)))
#out: tensor(True)
```

矩阵乘法

```python
torch.mm #只适用于二维tensor
torch.matmul
@

a=torch.ones(2,2)
b=a*3
a@b
torch.mm(a,b)
torch.matmul(a,b)
#out:tensor([[6., 6.],
#            [6., 6.]])
#三者输出一致

#多维情况的矩阵相乘
a=torch.rand(4,3,28,64)
b=torch.rand(4,1,64,32)
torch.matmul(a,b).shape
#out:torch.Size([4, 3, 28, 32])
```

指数运算

```python
a=torch.full([2,2],3)
a.pow(2)#平方运算
a**2
#out: [[9,9],[9,9]]  两个输出一致

aa=a**2
aa.sqrt()#开方
aa**(0.5)

aa.rsqrt()#取平方后的倒数
```

对数运算

```python
a=torch.exp(torch.ones(2,2))
'''out:
tensor([[2.7183, 2.7183],
        [2.7183, 2.7183]])

'''
torch.log(a)#会输出2*2的1 #默认以e为底，log2、log10分别以2和10为底
```

近似运算

```python
a=torch.tensor(3.14)
a.floor(),a.ceil(),a.trunc(),a.frac()#分别是向下取整，向上取整，取整数部分，取小数部分
#out: (tensor(3.), tensor(4.), tensor(3.), tensor(0.1400))

a.round()#四舍五入
```

裁剪运算

```python
grad=torch.rand(2,3)*5
grad.max()#输出最大值
grad.median()#输出中位数
grad.clamp(10)#将所有小于10的改为10
grad.clamp(0,10)#将所有数限制在0~10
```

属性统计

```python
#范数 norm
a=torch.full([8],1.)#8个1的向量
b=a.view(2,4)
c=a.view(2,2,2)
a.norm(1),b.norm(1),c.norm(1) #1范数，就是所有矩阵元素绝对值的和，所以全输出8

a.norm(2),b.norm(2),c.norm(2) #2范数，就是所有矩阵元素的绝对值的平方和开根号，所以全输出 根号8 =2.82

b.norm(1,dim=1)#对第1维度的1范数 [4,4]
b.norm(2,dim=1)#对第1维度的2范数 [2,2]

#mean,sum,min,max,prod
a=torch.arange(8).view(2,4).float()*2
a.min(),a.max(),a.mean(),a.prod(),a.sum()# 最小值、最大值、平均值和累乘值（因为有0所以是0）还有求和
#out: (tensor(0.), tensor(14.), tensor(7.), tensor(0.), tensor(56.))
a.argmax(),a.argmin() #最大值和最小值的下标，注意是打平后的下标。
#out: (tensor(7), tensor(0))
#我们可以指定维度，来使其输出它在该维度的下标
a=torch.rand(3,3,4)
print(a)
a.argmax(dim=2)
'''out :
tensor([[1, 1, 3],
        [1, 2, 2],
        [3, 3, 0]])
'''

#dim,keepdim
a=torch.rand(4,10)
print(a.max(dim=1))#max会返回值和index，但是keepdim默认false会降低到二个维度，
print(a.max(dim=1,keepdim=True))#将keepdim设置维True会保持返回值的原本维度
'''out:
torch.return_types.max(
values=tensor([0.9039, 0.9087, 0.9798, 0.8735]),
indices=tensor([8, 2, 9, 1]))
torch.return_types.max(
values=tensor([[0.9039],
        [0.9087],
        [0.9798],
        [0.8735]]),
indices=tensor([[8],
        [2],
        [9],
        [1]]))
'''

#top-k k-th
a=torch.rand(4,10)
a.topk(5,dim=1)#会返回第一个维度中前五的数值和index

a.kthvalue(8,dim=1)#会返回第1个维度，第八小的值和坐标

```



两个tensor的比较

可以用`trnsor.equal()`和`tensor.eq()`两个方法来比较tensor是否相同。

```python
# 该方法用于比较两个tensor是否一样，一样则返回True否则为False
a = torch.tensor([1,2,3,4])
b = torch.tensor([1,2,3,4])
print(a.equal(b))    # 返回True

# 该方法用于主元素比较是否相等，相等则在对应位置返回True，否则为False
a = torch.tensor([1,2,2,3])
b = torch.tensor([2,2,3,3])
print(a.eq(b))  # 返回tensor([False,True,False,True]),与a==b返回的结果一样
torch.all(torch.eq(a,b))#可以对比所以有内容，都一样返回1，不是都一样返回0
```

tensor的比较也可以使用符号`>,>=,<,<=,!=,==`但是需要注意的是该操作会返回一个全为bool的tensor，用于表示每个元素是否符合运算符。

```python
a=torch.rand(4,4)
a>0.5
'''out:
tensor([[False, False, False,  True],
        [ True,  True, False,  True],
        [False,  True,  True, False],
        [ True, False,  True, False]])
'''
```

高阶操作

```python
#where
a=torch.rand(4,4)
print(a)
a0=torch.zeros(4,4)
a1=torch.ones(4,4)
torch.where(a>0.5,a1,a0)#a满足条件取a1中的值，不满足条件取a2中的值

#gather
a=torch.rand(4,10)
idx=a.topk(dim=1,k=3)
idx=idx[1]
print(idx)
label=torch.arange(10)+33
label=label.expand(4,10)
print(label)
torch.gather(label,dim=1,index=idx.long())#实际上gather就是在label中根据idx矩阵中的元素取值。
'''out:
tensor([[5, 2, 0],
        [0, 8, 5],
        [0, 4, 6],
        [9, 2, 1]])
tensor([[33, 34, 35, 36, 37, 38, 39, 40, 41, 42],
        [33, 34, 35, 36, 37, 38, 39, 40, 41, 42],
        [33, 34, 35, 36, 37, 38, 39, 40, 41, 42],
        [33, 34, 35, 36, 37, 38, 39, 40, 41, 42]])
tensor([[38, 35, 33],
        [33, 41, 38],
        [33, 37, 39],
        [42, 35, 34]])
'''

```

